{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158c1cf0-934a-4728-92ac-563eed175c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU, QuantIdentity\n",
    "from brevitas.quant import IntBias\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.quant import Int8ActPerTensorFloat, Uint8ActPerTensorFloat, Int32Bias\n",
    "from brevitas.quant import Int8WeightPerTensorFloat, Int8WeightPerChannelFloat\n",
    "from brevitas.quant.scaled_int import Int32Bias\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de71baa4-9620-4051-b617-36ec0839cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-bit weight quantization configurations\n",
    "class Common8bitWeightPerTensorQuant(Int8WeightPerTensorFloat):\n",
    "    scaling_min_val = 2e-16\n",
    "\n",
    "class Common8bitWeightPerChannelQuant(Int8WeightPerChannelFloat):\n",
    "    scaling_per_output_channel = True\n",
    "    scaling_min_val = 2e-16\n",
    "\n",
    "# 8-bit activation quantization configurations\n",
    "class Common8bitActQuant(Int8ActPerTensorFloat):\n",
    "    scaling_min_val = 2e-16\n",
    "    restrict_scaling_type = RestrictValueType.LOG_FP\n",
    "\n",
    "class Common8bitUintActQuant(Uint8ActPerTensorFloat):\n",
    "    scaling_min_val = 2e-16\n",
    "    restrict_scaling_type = RestrictValueType.LOG_FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b79b9d8-cf34-4ce3-85f7-5d214fcc924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLeNet5_8bit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedLeNet5_8bit, self).__init__()\n",
    "        \n",
    "        # First convolutional layer (8-bit)\n",
    "        self.conv1 = QuantConv2d(\n",
    "            1, 6, kernel_size=5, stride=1, padding=2,\n",
    "            weight_bit_width=8,\n",
    "            bias=True,\n",
    "            weight_quant=Common8bitWeightPerChannelQuant,\n",
    "            input_quant=Common8bitActQuant,\n",
    "            output_quant=Common8bitActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        self.relu1 = QuantReLU(\n",
    "            bit_width=8,\n",
    "            act_quant=Common8bitUintActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        \n",
    "        # First average pooling layer\n",
    "        self.avg_pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolutional layer (8-bit)\n",
    "        self.conv2 = QuantConv2d(\n",
    "            6, 16, kernel_size=5, stride=1, padding=0,\n",
    "            weight_bit_width=8,\n",
    "            bias=True,\n",
    "            weight_quant=Common8bitWeightPerChannelQuant,\n",
    "            input_quant=Common8bitActQuant,\n",
    "            output_quant=Common8bitActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        self.relu2 = QuantReLU(\n",
    "            bit_width=8,\n",
    "            act_quant=Common8bitUintActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        \n",
    "        # Second average pooling layer\n",
    "        self.avg_pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # First fully connected layer (8-bit)\n",
    "        self.fc1 = QuantLinear(\n",
    "            16 * 5 * 5, 120,\n",
    "            bias=True,\n",
    "            weight_bit_width=8,\n",
    "            weight_quant=Common8bitWeightPerTensorQuant,\n",
    "            input_quant=Common8bitActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        self.relu3 = QuantReLU(\n",
    "            bit_width=8,\n",
    "            act_quant=Common8bitUintActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        \n",
    "        # Second fully connected layer (8-bit)\n",
    "        self.fc2 = QuantLinear(\n",
    "            120, 84,\n",
    "            bias=True,\n",
    "            weight_bit_width=8,\n",
    "            weight_quant=Common8bitWeightPerTensorQuant,\n",
    "            input_quant=Common8bitActQuant,\n",
    "            output_quant=Common8bitActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        self.relu4 = QuantReLU(\n",
    "            bit_width=8,\n",
    "            act_quant=Common8bitUintActQuant,\n",
    "            return_quant_tensor=True)\n",
    "        \n",
    "        # Output layer (8-bit)\n",
    "        self.fc3 = QuantLinear(\n",
    "            84, 10,\n",
    "            bias=True,\n",
    "            weight_bit_width=8,\n",
    "            weight_quant=Common8bitWeightPerTensorQuant,\n",
    "            input_quant=Common8bitActQuant,\n",
    "            output_quant=Common8bitActQuant,\n",
    "            return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "     \n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.avg_pool1(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.avg_pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.relu4(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c746f1-30a7-4d2e-952c-d1a77622c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb4440ec-8f9a-4467-9f17-42866818dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4728cd9-f135-4b1c-8d60-6a038f3e217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({accuracy:.2f}%)\\n')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854f516e-64a8-4dd7-85cd-6e09090e24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lenet5_quantized8.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c6b5d2-4342-4dd5-98f5-fc1b8abe97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create model instance and move to device\n",
    "    model = QuantizedLeNet5_8bit()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "    train_loader, test_loader = load_data()\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(10):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        accuracy = test(model, device, test_loader, criterion)\n",
    "        \n",
    "        # Save best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "    \n",
    "    print(f\"Best test accuracy: {best_accuracy:.2f}%\")\n",
    "    \n",
    "    # Load the best model for ONNX export\n",
    "    model.load_state_dict(torch.load(model_name, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    return model, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d00cf227-826d-4e66-a0d6-bb645acede6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhammadzaky/.local/lib/python3.10/site-packages/torch/nn/modules/pooling.py:627: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  return F.avg_pool2d(input, self.kernel_size, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302951\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.981188\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.457877\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.297873\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.197571\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.224574\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.190298\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.097600\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.112246\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.159719\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.195688\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.012858\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.077751\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.063654\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.098796\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.023723\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.043579\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090556\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.026729\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.061570\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9820/10000 (98.20%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.223980\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.085993\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.149243\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.014639\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.017437\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.033293\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.050564\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.038270\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.009062\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.022338\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9877/10000 (98.77%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.018221\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.008282\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.016209\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.043668\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.033381\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.009104\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.078310\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.012478\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.010048\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.111584\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9884/10000 (98.84%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.137365\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009663\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.010420\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.001590\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.007622\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.029964\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.012515\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.007726\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.021591\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.008464\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9885/10000 (98.85%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007681\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.010401\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.079605\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.040644\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.019142\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.002803\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.009176\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.017908\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.005229\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.021909\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9891/10000 (98.91%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.066999\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.006696\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.048276\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.066971\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.003649\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.048467\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.008543\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002888\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.044144\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.025217\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002223\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.005695\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.014752\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.038671\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.059310\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.044707\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.111994\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.020953\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.028488\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.000869\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9892/10000 (98.92%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.179870\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.002757\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.065430\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001775\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.038152\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.026620\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.038496\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.011355\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.044210\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003502\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9904/10000 (99.04%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.003270\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.044746\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000610\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.010244\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.043219\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.013846\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001464\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.056188\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.012514\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.014106\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9890/10000 (98.90%)\n",
      "\n",
      "Best test accuracy: 99.09%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492e0147-78de-42e1-acb1-ca5012ce593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_onnx_qcdq\n",
    "import qonnx.util.cleanup\n",
    "model = QuantizedLeNet5_8bit()\n",
    "state_dict = torch.load('lenet5_quantized8.pth', map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model_filename = \"lenet5_test8.onnx\"\n",
    "export_onnx_qcdq(model, input_shape=([1,1,28,28]), export_path=(model_filename), opset_version=13)\n",
    "model = qonnx.util.cleanup.cleanup(in_file=model_filename, out_file=model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b270715-748e-443d-a318-74ea871acd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
